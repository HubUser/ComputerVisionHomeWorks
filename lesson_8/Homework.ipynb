{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOMEWORK 8\n",
    "\n",
    "In this homework you are going to implement your first machine learning algorithm to automatically binarize document images. The goal of document binarization is to separate the characters (letters) from everything else. This is the crucial part for automatic document understanding and information extraction from the . In order to do so, you will use the Otsu thresholding algorithm.\n",
    "\n",
    "At the end of this notebook, there are a couple of questions for you to answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [15, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the document image we will be working on in this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('./data/document.jpg')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "plt.imshow(img, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's have a look at the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "h = np.histogram(img, 256)\n",
    "plt.bar(h[1][0:-1], h[0])\n",
    "plt.xlabel('Colour'), plt.ylabel('Count')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otsu Thresholding\n",
    "\n",
    "Let's now implement the Otsu thresholding algorithm. Remember that the algorithm consists of an optimization process that finds the thresholds that minimize the intra-class variance or, equivalently, maximize the inter-class variance.\n",
    "\n",
    "In this homework, you are going to demonstrate the working principle of the Otsu algorithm. Therefore, you won't have to worry about an efficient implementation, we are going to use the brute force approach here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal threshold 160\n"
     ]
    }
   ],
   "source": [
    "# Get image dimensions\n",
    "rows, cols = img.shape\n",
    "# Compute the total amount of image pixels\n",
    "num_pixels = rows * cols\n",
    "\n",
    "# Initializations\n",
    "best_wcv = 1e6  # Best within-class variance (wcv)\n",
    "opt_th = None   # Threshold corresponding to the best wcv\n",
    "\n",
    "# Brute force search using all possible thresholds (levels of gray)\n",
    "for th in range(0, 256):\n",
    "    # Extract the image pixels corresponding to the background\n",
    "    foreground = img[img >= th]\n",
    "    # Extract the image pixels corresponding to the background\n",
    "    background = img[img < th]\n",
    "    \n",
    "    # If foreground or background are empty, continue\n",
    "    if len(foreground) == 0 or len(background) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Compute class-weights (omega parameters) for foreground and background\n",
    "    omega_f = len(foreground) / num_pixels\n",
    "    omega_b = len(background) / num_pixels\n",
    "    \n",
    "    # Compute pixel variance for foreground and background\n",
    "    # Hint: Check out the var function from numpy ;-)\n",
    "    # https://numpy.org/doc/stable/reference/generated/numpy.var.html\n",
    "    sigma2_f = np.var(foreground)\n",
    "    sigma2_b = np.var(background)\n",
    "    \n",
    "    # Compute the within-class variance\n",
    "    wcv = omega_f * sigma2_f + omega_b * sigma2_b\n",
    "    \n",
    "    # Perform the optimization\n",
    "    if wcv < best_wcv:\n",
    "        best_wcv = wcv\n",
    "        opt_th = th\n",
    "        \n",
    "# Print out the optimal threshold found by Otsu algorithm\n",
    "print('Optimal threshold', opt_th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's compare the original image and its thresholded representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.subplot(121), plt.imshow(img, cmap='gray')\n",
    "plt.subplot(122), plt.imshow(img > opt_th, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "* Looking at the computed histogram, could it be considered bimodal?\n",
    "* Looking at the computed histogram, what binarization threshold would you chose? Why?\n",
    "* Looking at the resulting (thresholded) image, is the text binarization (detection) good?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Looking at the computed histogram, could it be considered bimodal?\n",
    "\n",
    "The distribution contains two peaks at `130` and `200`. Two Gaussian distributions with same `\\mu`s and reasonable sigmas have much lower values between their peaks. That might mean pixels with levels between `150` and `170` might be incorrectly splitted on classes. \n",
    "\n",
    "And indeed, we see text in the upper part between the columns is hardly readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [0.4, 0.6]\n",
    "centers = [130, 200]\n",
    "sigmas = [18, 10]\n",
    "\n",
    "xx = np.linspace(0, 255, 256)\n",
    "mixture = np.zeros_like(xx)\n",
    "\n",
    "for wk, mu, s in zip(weights, centers, sigmas):\n",
    "    mixture += wk * norm.pdf(xx, loc=mu, scale=s)\n",
    "\n",
    "plt.bar(h[1][0:-1], h[0])\n",
    "plt.xlabel('Colour'), plt.ylabel('Count')\n",
    "plt.grid(True)\n",
    "plt.plot(xx, mixture * np.sum(h[0]) / 1.8, 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Looking at the computed histogram, what binarization threshold would you chose? Why?\n",
    "\n",
    "To be honest, I have no idea why any other threashold is better than `160` just from looking to histogram. I would try to lower the threashold by `5-10` levels to try to make text in the middle more readable.\n",
    "\n",
    "> Looking at the resulting (thresholded) image, is the text binarization (detection) good?\n",
    "\n",
    "It is not good everywhere. I would try to use adaptive threshold approach (it is marginally better):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.filters import threshold_sauvola\n",
    "\n",
    "th = threshold_sauvola(img, window_size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(121), plt.imshow(img > opt_th, cmap='gray')\n",
    "plt.subplot(122), plt.imshow(img > th, cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
